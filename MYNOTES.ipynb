{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# MIS NOTAS PERSONALES EN ANALÍTICA DE DATOS\n",
    "\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <h1><span style=\"color:red;\">EL ICEBERG DE LA REGRESIÓN LINEAL</span></h1>\n",
    "</div>\n",
    "\n",
    "\n",
    "## El coeficiente de Correlación Lineal R\n",
    "\n",
    "Considere cinco deficiniciones que sumergen a diferente profundidad el conocimiento que aporta el coeficiente R.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<hr style=\"border: none; height: 3px; background: linear-gradient(to right, #007bff, #00c6ff); width: 100%;\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> <!-- Salto de línea -->\n",
    "**NIVEL 1:LA INTERPRETACIÓN 101** El coeficiente R mide la intensidad de la relación lineal entre dos variables. Varía entre 1 y -1, teniendo que el cero indica no hay relación. Esto se puede entender como qué tan bien una línea puede explicar una variable dependiente $ y_{i} $ a partir de una variable independiente $ x_{i} $.\n",
    "\n",
    "Fórmula de Pearson:\n",
    "\n",
    "$$ r = \\frac{1}{n-1} \\sum _{i} \\left(\\frac{X_i - \\bar{X}}{S_x}\\right)\\left(\\frac{Y_i - \\bar{Y}}{S_y}\\right) $$\n",
    "\n",
    "NOTA: Tenga en cuenta que esta es una simplificación de la obtención de dicho coeficiente, ella facilita la obtención del r a través de tablas.\n",
    "\n",
    "<br> <!-- Salto de línea -->\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <table style=\"margin: auto;\">\n",
    "        <tr>\n",
    "            <th>X (Variable Independiente)</th>\n",
    "            <th>Y (Variable Dependiente)</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>1</td>\n",
    "            <td>2</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>2</td>\n",
    "            <td>3</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>3</td>\n",
    "            <td>5</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>4</td>\n",
    "            <td>4</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>5</td>\n",
    "            <td>6</td>\n",
    "        </tr>\n",
    "    </table>\n",
    "\n",
    "<br> <!-- Salto de línea -->\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <ul style=\"list-style-position: inside;\">\n",
    "        <li>Media X: 3.0000</li>\n",
    "        <li>Media Y: 4.0000</li>\n",
    "        <li>Desviación Standard X: 1.5811</li>\n",
    "        <li>Desviación Standard Y: 1.5811</li>\n",
    "    </ul>\n",
    "</div>\n",
    "\n",
    "\n",
    "<br> <!-- Salto de línea -->\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"./images/pearson.png\" alt=\"Gráfico de la Correlación de Pearson\" width=\"400\" height=\"300\">\n",
    "</div>\n",
    "\n",
    "<br> <!-- Salto de línea -->\n",
    "<br> <!-- Salto de línea -->\n",
    "<br> <!-- Salto de línea -->\n",
    "<hr style=\"border: none; height: 3px; background: linear-gradient(to right, #007bff, #00c6ff); width: 100%;\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NIVEL 2: INTERPRETACIÓN GEOMÉTRICA** Considere el origen geométrico del coeficiente r. El origen del r es un algoritmo de mínimos cuadrados que minimiza el área cuadrada de todos los puntos para conformar una recta que sería la que pasa más cerca por todos ellos minimizando la distancia punto-recta. Este origen incluye las cantidades DAM, RSE, RSME. La comprensión geométrica busca que el residuo de todas las distancias de punto a linea sea el menor posible y con esa línea, el valor es la pendiente m que multiplica al $ x_{i} $ del modelo. Las presunciónes tomadas para completar la minimización de las series son conocidas como las de Gauss y serán exploradas en el siguiente nivel.\n",
    "\n",
    "### Diferencia Absoluta Media (DAM)\n",
    "La **DAM** mide el promedio de las diferencias absolutas entre los valores observados y los valores ajustados por el modelo:\n",
    "$$\n",
    "DAM = \\frac{1}{n} \\sum_{i=1}^{n} | y_i - \\hat{y}_i |\n",
    "$$\n",
    "\n",
    "### Residuo Estándar del Error (RSE)\n",
    "El **RSE** indica qué tan bien el modelo de regresión representa los datos, basado en la raíz cuadrada del error cuadrático medio corregido:\n",
    "$$\n",
    "RSE = \\sqrt{\\frac{1}{n-2} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}\n",
    "$$\n",
    "\n",
    "### Raíz del Error Cuadrático Medio (RMSE)\n",
    "La **RMSE** es una métrica común en regresión que mide el promedio de los errores cuadráticos:\n",
    "$$\n",
    "RMSE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}\n",
    "$$\n",
    "\n",
    "### Pendiente \\( m \\) en regresión lineal:\n",
    "$$\n",
    "m = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum (x_i - \\bar{x})^2}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "\n",
    "<span style=\"color:blue;\">(Imagen por desarrollar, ejemplo ilustrativo)</span>\n",
    "\n",
    "<img src=\"./images/pendientegeometrica.png\" alt=\"Descripción\" width=\"900\" height=\"700\">\n",
    "</div>\n",
    "\n",
    "<hr style=\"border: none; height: 3px; background: linear-gradient(to right, #007bff, #00c6ff); width: 100%;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NIVEL 3:LA PERSPECTIVA DE LA OPTIMIZACIÓN** Como se mencionó anteriormente, el origen del r es un algoritmo de mínimos cuadrados que minimiza el área cuadrada de todos los puntos para conformar una recta que sería la que pasa más cerca por todos ellos minimizando la distancia punto-recta. Esta interpretación implica que al final, se obtiene de un procedimiento de MLE (O en español, estimación de máxima verosimilitud EMV). Es decir, es un método estadístico para estimar los parámetros de un modelo probabilístico. El método de mínimos cuadrados emerge naturalmente de esta metodología al encontrar la pendiente de la línea de mejor ajuste en unidades estandarizadas.\n",
    "\n",
    "$$\n",
    "S = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "- \\( y_i \\) es el valor observado.\n",
    "- \\( \\hat{y}_i \\) es el valor predicho por el modelo.\n",
    "\n",
    "\n",
    "$$\n",
    "m = \\frac{\\sum_{i} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i} (x_i - \\bar{x})^2}\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "- \\( \\bar{x} \\) y \\( \\bar{y} \\) son las medias de \\( x \\) y \\( y \\).\n",
    "\n",
    "Después de calcular \\( m \\), el intercepto \\( b \\) es:\n",
    "\n",
    "$$\n",
    "b = \\bar{y} - m \\bar{x}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "L(m, b, \\sigma^2) = \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left( -\\frac{(y_i - (m x_i + b))^2}{2 \\sigma^2} \\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(m, b, \\sigma^2) = -\\sum_{i=1}^{n} \\left(\\frac{(y_i - (m x_i + b))^2}{2 \\sigma^2} + \\frac{1}{2} \\log (2 \\pi \\sigma^2) \\right)\n",
    "$$\n",
    "\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "\n",
    "<span style=\"color:blue;\">(Imagen por desarrollar, ejemplo ilustrativo)</span>\n",
    "\n",
    "<img src=\"./images/pendiente-opt.jpg\" alt=\"Descripción\" width=\"300\" height=\"200\">\n",
    "</div>\n",
    "\n",
    "<hr style=\"border: none; height: 3px; background: linear-gradient(to right, #007bff, #00c6ff); width: 100%;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NIVEL 4:LA VERSIÓN COMPUTACIONAL** Una vez se tomaron las consideraciones de los niveles anteriores, se puede resumir el problema de forma algebráica en encontrar una matriz de covarianzas cuyo coeficiente fuera de la diagonal principal es la covarianza de una variable respecto de la otra. Al aplicar un determinante se resulta con la fórmula:\n",
    "\n",
    "\n",
    "$$\n",
    "r = \\frac{\\text{Cov}(X,Y)}{\\sigma_X \\sigma_Y}\n",
    "$$\n",
    "\n",
    "De manera que el cálculo computacional de diversas funciones como las de EXCEL y librerías de Python se reducen a esta operación.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "\n",
    "<span style=\"color:blue;\">(Imagen por desarrollar, ejemplo ilustrativo)</span>\n",
    "\n",
    "<img src=\"RIgsS.png\" alt=\"Descripción\" width=\"300\" height=\"200\">\n",
    "</div>\n",
    "\n",
    "\n",
    "<hr style=\"border: none; height: 3px; background: linear-gradient(to right, #007bff, #00c6ff); width: 100%;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NIVEL 5:El significado VECTORIAL (Proyecciones y vectores en el espacio)** \n",
    "\n",
    "Una vez aplicados los arreglos algebráicos se empieza a notar un aspecto más profundo y fundamental relacionado con las componentes y el sistema coordenado mismo de la representación. Se obtiene que r es el coseno del ángulo entre dos vectores X y Y centrados, en el contexto de un espacio n-dimensional. En este caso da que es la proporción de qué tanto se desvían las desviaciones Sx y Sy de sus respectivas medias. Por eso da la proporción de varianza explicada, ya que se obtiene de un coseno cuadrado en un espacio de Hilbert.\n",
    "\n",
    "\n",
    "$$\n",
    "r = \\cos \\theta = \\frac{\\langle X - \\bar{X}, Y - \\bar{Y} \\rangle}{\\|X - \\bar{X}\\| \\cdot \\|Y - \\bar{Y}\\|}\n",
    "$$\n",
    "\n",
    "<hr style=\"border: none; height: 3px; background: linear-gradient(to right, #007bff, #00c6ff); width: 100%;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**FOSA DE LAS MARIANAS: BONUS TRACK**\n",
    "\n",
    "El coeficiente de correlación r sólo captura relación lineal, puede ser cero ante relaciones no lineares muy fuertes.\n",
    "\n",
    "Para poder hacer todas estas inferencias, no olvidemos que se presume que los datos tienen homocedasticidad y además cumplen la distribución normal.\n",
    "\n",
    "El coeficiente de Spearman $ /rho $ o el de Kendall $ /tao $ puede ser mejor para detectar relaciones monónotas pero no estrictamente lineales.\n",
    "\n",
    "El \"coeficiente de correlación simple\" es, en realidad, una puerta de entrada al análisis de regresión, la optimización, el álgebra lineal e incluso el aprendizaje automático (donde conceptos como R² y MSE dominan). La mayoría de las explicaciones nunca van más allá del Nivel 2.\n",
    "\n",
    "En este punto podemos seguir descendiendo en cráteres y lagos que nos sumergen hasta Estadísticas Robustas y teoría de la información.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
